{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3dd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8266f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_X_data = Variable(torch.Tensor([[1.0,4 , 6], [2.0, 0.6, 1.7], [3.0, 3.5, 7], [1.3,5.4 , 8.6], [2.8, 9.6, 5.47], [3.6, 7.5, 0.7]]))\n",
    "\n",
    "Sample_Y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0], [2.5], [6.0], [8.0]]))\n",
    "\n",
    "#or use\n",
    "Sample_X_data = torch.randn(6, 3, requires_grad=True)\n",
    "Sample_Y_data = torch.randn(6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15493e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6751c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9749, -1.1847,  0.1703],\n",
       "        [-2.1798,  1.3326, -2.1643],\n",
       "        [ 0.0183, -0.5771,  2.4744],\n",
       "        [ 0.5177,  0.9861, -0.1627],\n",
       "        [-1.0237,  2.1725, -0.4083],\n",
       "        [ 0.1949, -1.6750,  0.7306]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158869f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4304],\n",
       "        [-0.5102],\n",
       "        [ 0.7522],\n",
       "        [ 2.8121],\n",
       "        [ 0.0910],\n",
       "        [-0.0444]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8311728",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDim = 3\n",
    "OutputDim = 1\n",
    "class LinearRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self): \n",
    "        super(LinearRegression, self).__init__() \n",
    "        self.linear = torch.nn.Linear(InputDim, OutputDim)  \n",
    "    def forward(self, x): \n",
    "        predict_y = self.linear(x) \n",
    "        return predict_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d3aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab3babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define_criterion = torch.nn.MSELoss(size_average=False) replace with:\n",
    "criterion = torch.nn.MSELoss( reduction='sum')\n",
    "Optimizer = torch.optim.SGD(linear_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ba4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7507bf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss function 3.6271495819091797\n",
      "epoch 1, loss function 3.624025344848633\n",
      "epoch 2, loss function 3.621199131011963\n",
      "epoch 3, loss function 3.618640661239624\n",
      "epoch 4, loss function 3.616326332092285\n",
      "epoch 5, loss function 3.614232063293457\n",
      "epoch 6, loss function 3.612337112426758\n",
      "epoch 7, loss function 3.610621690750122\n",
      "epoch 8, loss function 3.609070301055908\n",
      "epoch 9, loss function 3.607666492462158\n",
      "epoch 10, loss function 3.6063954830169678\n",
      "epoch 11, loss function 3.6052451133728027\n",
      "epoch 12, loss function 3.6042046546936035\n",
      "epoch 13, loss function 3.6032631397247314\n",
      "epoch 14, loss function 3.6024112701416016\n",
      "epoch 15, loss function 3.601639747619629\n",
      "epoch 16, loss function 3.6009416580200195\n",
      "epoch 17, loss function 3.600311040878296\n",
      "epoch 18, loss function 3.5997390747070312\n",
      "epoch 19, loss function 3.599222183227539\n",
      "epoch 20, loss function 3.5987539291381836\n",
      "epoch 21, loss function 3.598330020904541\n",
      "epoch 22, loss function 3.597947120666504\n",
      "epoch 23, loss function 3.597599506378174\n",
      "epoch 24, loss function 3.5972859859466553\n",
      "epoch 25, loss function 3.597001791000366\n",
      "epoch 26, loss function 3.5967445373535156\n",
      "epoch 27, loss function 3.5965118408203125\n",
      "epoch 28, loss function 3.5963010787963867\n",
      "epoch 29, loss function 3.5961103439331055\n",
      "epoch 30, loss function 3.595938205718994\n",
      "epoch 31, loss function 3.59578275680542\n",
      "epoch 32, loss function 3.5956406593322754\n",
      "epoch 33, loss function 3.595513105392456\n",
      "epoch 34, loss function 3.595397710800171\n",
      "epoch 35, loss function 3.5952932834625244\n",
      "epoch 36, loss function 3.595198154449463\n",
      "epoch 37, loss function 3.5951120853424072\n",
      "epoch 38, loss function 3.595034599304199\n",
      "epoch 39, loss function 3.5949645042419434\n",
      "epoch 40, loss function 3.594900608062744\n",
      "epoch 41, loss function 3.5948429107666016\n",
      "epoch 42, loss function 3.5947909355163574\n",
      "epoch 43, loss function 3.5947442054748535\n",
      "epoch 44, loss function 3.5947012901306152\n",
      "epoch 45, loss function 3.594663143157959\n",
      "epoch 46, loss function 3.59462833404541\n",
      "epoch 47, loss function 3.5945959091186523\n",
      "epoch 48, loss function 3.5945675373077393\n",
      "epoch 49, loss function 3.594541549682617\n",
      "epoch 50, loss function 3.5945186614990234\n",
      "epoch 51, loss function 3.594496726989746\n",
      "epoch 52, loss function 3.594478130340576\n",
      "epoch 53, loss function 3.5944604873657227\n",
      "epoch 54, loss function 3.594444990158081\n",
      "epoch 55, loss function 3.594430685043335\n",
      "epoch 56, loss function 3.5944180488586426\n",
      "epoch 57, loss function 3.5944056510925293\n",
      "epoch 58, loss function 3.5943961143493652\n",
      "epoch 59, loss function 3.594386100769043\n",
      "epoch 60, loss function 3.594377279281616\n",
      "epoch 61, loss function 3.594369649887085\n",
      "epoch 62, loss function 3.594362735748291\n",
      "epoch 63, loss function 3.5943565368652344\n",
      "epoch 64, loss function 3.5943498611450195\n",
      "epoch 65, loss function 3.5943450927734375\n",
      "epoch 66, loss function 3.5943403244018555\n",
      "epoch 67, loss function 3.5943360328674316\n",
      "epoch 68, loss function 3.594331741333008\n",
      "epoch 69, loss function 3.5943286418914795\n",
      "epoch 70, loss function 3.594325542449951\n",
      "epoch 71, loss function 3.5943222045898438\n",
      "epoch 72, loss function 3.594320058822632\n",
      "epoch 73, loss function 3.5943171977996826\n",
      "epoch 74, loss function 3.594315767288208\n",
      "epoch 75, loss function 3.594313621520996\n",
      "epoch 76, loss function 3.5943119525909424\n",
      "epoch 77, loss function 3.5943102836608887\n",
      "epoch 78, loss function 3.594308614730835\n",
      "epoch 79, loss function 3.5943074226379395\n",
      "epoch 80, loss function 3.594306230545044\n",
      "epoch 81, loss function 3.5943055152893066\n",
      "epoch 82, loss function 3.5943045616149902\n",
      "epoch 83, loss function 3.5943033695220947\n",
      "epoch 84, loss function 3.5943026542663574\n",
      "epoch 85, loss function 3.594301223754883\n",
      "epoch 86, loss function 3.5943009853363037\n",
      "epoch 87, loss function 3.5943005084991455\n",
      "epoch 88, loss function 3.5943002700805664\n",
      "epoch 89, loss function 3.59429931640625\n",
      "epoch 90, loss function 3.594299554824829\n",
      "epoch 91, loss function 3.594298839569092\n",
      "epoch 92, loss function 3.594298839569092\n",
      "epoch 93, loss function 3.594298839569092\n",
      "epoch 94, loss function 3.5942978858947754\n",
      "epoch 95, loss function 3.5942976474761963\n",
      "epoch 96, loss function 3.594297409057617\n",
      "epoch 97, loss function 3.594297170639038\n",
      "epoch 98, loss function 3.594297170639038\n",
      "epoch 99, loss function 3.594296932220459\n",
      "epoch 100, loss function 3.59429669380188\n",
      "epoch 101, loss function 3.594296455383301\n",
      "epoch 102, loss function 3.5942959785461426\n",
      "epoch 103, loss function 3.594296455383301\n",
      "epoch 104, loss function 3.5942962169647217\n",
      "epoch 105, loss function 3.5942962169647217\n",
      "epoch 106, loss function 3.5942959785461426\n",
      "epoch 107, loss function 3.5942959785461426\n",
      "epoch 108, loss function 3.5942952632904053\n",
      "epoch 109, loss function 3.5942959785461426\n",
      "epoch 110, loss function 3.594295024871826\n",
      "epoch 111, loss function 3.5942955017089844\n",
      "epoch 112, loss function 3.5942959785461426\n",
      "epoch 113, loss function 3.5942955017089844\n",
      "epoch 114, loss function 3.5942955017089844\n",
      "epoch 115, loss function 3.5942955017089844\n",
      "epoch 116, loss function 3.5942955017089844\n",
      "epoch 117, loss function 3.5942955017089844\n",
      "epoch 118, loss function 3.594295024871826\n",
      "epoch 119, loss function 3.594295024871826\n",
      "epoch 120, loss function 3.5942952632904053\n",
      "epoch 121, loss function 3.5942955017089844\n",
      "epoch 122, loss function 3.594295024871826\n",
      "epoch 123, loss function 3.5942955017089844\n",
      "epoch 124, loss function 3.594295024871826\n",
      "epoch 125, loss function 3.5942952632904053\n",
      "epoch 126, loss function 3.594295024871826\n",
      "epoch 127, loss function 3.5942955017089844\n",
      "epoch 128, loss function 3.5942955017089844\n",
      "epoch 129, loss function 3.5942952632904053\n",
      "epoch 130, loss function 3.5942955017089844\n",
      "epoch 131, loss function 3.594294786453247\n",
      "epoch 132, loss function 3.594295024871826\n",
      "epoch 133, loss function 3.5942959785461426\n",
      "epoch 134, loss function 3.594295024871826\n",
      "epoch 135, loss function 3.594294786453247\n",
      "epoch 136, loss function 3.594295024871826\n",
      "epoch 137, loss function 3.594295024871826\n",
      "epoch 138, loss function 3.594295024871826\n",
      "epoch 139, loss function 3.594295024871826\n",
      "epoch 140, loss function 3.5942955017089844\n",
      "epoch 141, loss function 3.5942955017089844\n",
      "epoch 142, loss function 3.594295024871826\n",
      "epoch 143, loss function 3.594294548034668\n",
      "epoch 144, loss function 3.594295024871826\n",
      "epoch 145, loss function 3.5942955017089844\n",
      "epoch 146, loss function 3.594295024871826\n",
      "epoch 147, loss function 3.5942955017089844\n",
      "epoch 148, loss function 3.594295024871826\n",
      "epoch 149, loss function 3.594295024871826\n",
      "epoch 150, loss function 3.5942952632904053\n",
      "epoch 151, loss function 3.594295024871826\n",
      "epoch 152, loss function 3.5942955017089844\n",
      "epoch 153, loss function 3.594295024871826\n",
      "epoch 154, loss function 3.5942952632904053\n",
      "epoch 155, loss function 3.594295024871826\n",
      "epoch 156, loss function 3.594295024871826\n",
      "epoch 157, loss function 3.594294786453247\n",
      "epoch 158, loss function 3.594295024871826\n",
      "epoch 159, loss function 3.594294786453247\n",
      "epoch 160, loss function 3.594295024871826\n",
      "epoch 161, loss function 3.594294786453247\n",
      "epoch 162, loss function 3.5942952632904053\n",
      "epoch 163, loss function 3.5942955017089844\n",
      "epoch 164, loss function 3.594294548034668\n",
      "epoch 165, loss function 3.5942955017089844\n",
      "epoch 166, loss function 3.594295024871826\n",
      "epoch 167, loss function 3.594294548034668\n",
      "epoch 168, loss function 3.5942952632904053\n",
      "epoch 169, loss function 3.5942955017089844\n",
      "epoch 170, loss function 3.594295024871826\n",
      "epoch 171, loss function 3.5942955017089844\n",
      "epoch 172, loss function 3.5942955017089844\n",
      "epoch 173, loss function 3.5942952632904053\n",
      "epoch 174, loss function 3.5942955017089844\n",
      "epoch 175, loss function 3.5942952632904053\n",
      "epoch 176, loss function 3.594295024871826\n",
      "epoch 177, loss function 3.5942955017089844\n",
      "epoch 178, loss function 3.5942955017089844\n",
      "epoch 179, loss function 3.594295024871826\n",
      "epoch 180, loss function 3.5942952632904053\n",
      "epoch 181, loss function 3.5942952632904053\n",
      "epoch 182, loss function 3.5942952632904053\n",
      "epoch 183, loss function 3.5942952632904053\n",
      "epoch 184, loss function 3.5942955017089844\n",
      "epoch 185, loss function 3.5942952632904053\n",
      "epoch 186, loss function 3.594295024871826\n",
      "epoch 187, loss function 3.5942952632904053\n",
      "epoch 188, loss function 3.5942955017089844\n",
      "epoch 189, loss function 3.594295024871826\n",
      "epoch 190, loss function 3.5942955017089844\n",
      "epoch 191, loss function 3.594295024871826\n",
      "epoch 192, loss function 3.5942955017089844\n",
      "epoch 193, loss function 3.5942952632904053\n",
      "epoch 194, loss function 3.5942955017089844\n",
      "epoch 195, loss function 3.5942952632904053\n",
      "epoch 196, loss function 3.594295024871826\n",
      "epoch 197, loss function 3.594295024871826\n",
      "epoch 198, loss function 3.5942955017089844\n",
      "epoch 199, loss function 3.594295024871826\n",
      "epoch 200, loss function 3.594295024871826\n",
      "epoch 201, loss function 3.594295024871826\n",
      "epoch 202, loss function 3.594295024871826\n",
      "epoch 203, loss function 3.594295024871826\n",
      "epoch 204, loss function 3.594294786453247\n",
      "epoch 205, loss function 3.594295024871826\n",
      "epoch 206, loss function 3.5942955017089844\n",
      "epoch 207, loss function 3.5942955017089844\n",
      "epoch 208, loss function 3.594295024871826\n",
      "epoch 209, loss function 3.5942955017089844\n",
      "epoch 210, loss function 3.5942955017089844\n",
      "epoch 211, loss function 3.594295024871826\n",
      "epoch 212, loss function 3.594295024871826\n",
      "epoch 213, loss function 3.594295024871826\n",
      "epoch 214, loss function 3.594295024871826\n",
      "epoch 215, loss function 3.5942955017089844\n",
      "epoch 216, loss function 3.5942955017089844\n",
      "epoch 217, loss function 3.594295024871826\n",
      "epoch 218, loss function 3.5942955017089844\n",
      "epoch 219, loss function 3.5942955017089844\n",
      "epoch 220, loss function 3.5942955017089844\n",
      "epoch 221, loss function 3.594295024871826\n",
      "epoch 222, loss function 3.594295024871826\n",
      "epoch 223, loss function 3.594295024871826\n",
      "epoch 224, loss function 3.5942955017089844\n",
      "epoch 225, loss function 3.5942955017089844\n",
      "epoch 226, loss function 3.5942955017089844\n",
      "epoch 227, loss function 3.5942955017089844\n",
      "epoch 228, loss function 3.5942952632904053\n",
      "epoch 229, loss function 3.594295024871826\n",
      "epoch 230, loss function 3.5942952632904053\n",
      "epoch 231, loss function 3.5942952632904053\n",
      "epoch 232, loss function 3.594295024871826\n",
      "epoch 233, loss function 3.594295024871826\n",
      "epoch 234, loss function 3.594295024871826\n",
      "epoch 235, loss function 3.594295024871826\n",
      "epoch 236, loss function 3.594295024871826\n",
      "epoch 237, loss function 3.594295024871826\n",
      "epoch 238, loss function 3.5942952632904053\n",
      "epoch 239, loss function 3.5942955017089844\n",
      "epoch 240, loss function 3.5942952632904053\n",
      "epoch 241, loss function 3.5942955017089844\n",
      "epoch 242, loss function 3.5942957401275635\n",
      "epoch 243, loss function 3.5942957401275635\n",
      "epoch 244, loss function 3.5942955017089844\n",
      "epoch 245, loss function 3.5942955017089844\n",
      "epoch 246, loss function 3.5942957401275635\n",
      "epoch 247, loss function 3.5942957401275635\n",
      "epoch 248, loss function 3.5942957401275635\n",
      "epoch 249, loss function 3.5942957401275635\n",
      "epoch 250, loss function 3.5942957401275635\n",
      "epoch 251, loss function 3.5942957401275635\n",
      "epoch 252, loss function 3.5942957401275635\n",
      "epoch 253, loss function 3.5942957401275635\n",
      "epoch 254, loss function 3.5942957401275635\n",
      "epoch 255, loss function 3.5942957401275635\n",
      "epoch 256, loss function 3.5942957401275635\n",
      "epoch 257, loss function 3.5942957401275635\n",
      "epoch 258, loss function 3.5942957401275635\n",
      "epoch 259, loss function 3.5942957401275635\n",
      "epoch 260, loss function 3.5942957401275635\n",
      "epoch 261, loss function 3.5942957401275635\n",
      "epoch 262, loss function 3.5942957401275635\n",
      "epoch 263, loss function 3.5942957401275635\n",
      "epoch 264, loss function 3.5942957401275635\n",
      "epoch 265, loss function 3.5942957401275635\n",
      "epoch 266, loss function 3.5942957401275635\n",
      "epoch 267, loss function 3.5942957401275635\n",
      "epoch 268, loss function 3.5942957401275635\n",
      "epoch 269, loss function 3.5942957401275635\n",
      "epoch 270, loss function 3.5942957401275635\n",
      "epoch 271, loss function 3.5942957401275635\n",
      "epoch 272, loss function 3.5942957401275635\n",
      "epoch 273, loss function 3.5942957401275635\n",
      "epoch 274, loss function 3.5942957401275635\n",
      "epoch 275, loss function 3.5942957401275635\n",
      "epoch 276, loss function 3.5942957401275635\n",
      "epoch 277, loss function 3.5942957401275635\n",
      "epoch 278, loss function 3.5942957401275635\n",
      "epoch 279, loss function 3.5942957401275635\n",
      "epoch 280, loss function 3.5942957401275635\n",
      "epoch 281, loss function 3.5942957401275635\n",
      "epoch 282, loss function 3.5942957401275635\n",
      "epoch 283, loss function 3.5942957401275635\n",
      "epoch 284, loss function 3.5942957401275635\n",
      "epoch 285, loss function 3.5942957401275635\n",
      "epoch 286, loss function 3.5942957401275635\n",
      "epoch 287, loss function 3.5942957401275635\n",
      "epoch 288, loss function 3.5942957401275635\n",
      "epoch 289, loss function 3.5942957401275635\n",
      "epoch 290, loss function 3.5942957401275635\n",
      "epoch 291, loss function 3.5942957401275635\n",
      "epoch 292, loss function 3.5942957401275635\n",
      "epoch 293, loss function 3.5942957401275635\n",
      "epoch 294, loss function 3.5942957401275635\n",
      "epoch 295, loss function 3.5942957401275635\n",
      "epoch 296, loss function 3.5942957401275635\n",
      "epoch 297, loss function 3.5942957401275635\n",
      "epoch 298, loss function 3.5942957401275635\n",
      "epoch 299, loss function 3.5942957401275635\n",
      "epoch 300, loss function 3.5942957401275635\n",
      "epoch 301, loss function 3.5942957401275635\n",
      "epoch 302, loss function 3.5942957401275635\n",
      "epoch 303, loss function 3.5942957401275635\n",
      "epoch 304, loss function 3.5942957401275635\n",
      "epoch 305, loss function 3.5942957401275635\n",
      "epoch 306, loss function 3.5942957401275635\n",
      "epoch 307, loss function 3.5942957401275635\n",
      "epoch 308, loss function 3.5942957401275635\n",
      "epoch 309, loss function 3.5942957401275635\n",
      "epoch 310, loss function 3.5942957401275635\n",
      "epoch 311, loss function 3.5942957401275635\n",
      "epoch 312, loss function 3.5942957401275635\n",
      "epoch 313, loss function 3.5942957401275635\n",
      "epoch 314, loss function 3.5942957401275635\n",
      "epoch 315, loss function 3.5942957401275635\n",
      "epoch 316, loss function 3.5942957401275635\n",
      "epoch 317, loss function 3.5942957401275635\n",
      "epoch 318, loss function 3.5942957401275635\n",
      "epoch 319, loss function 3.5942957401275635\n",
      "epoch 320, loss function 3.5942957401275635\n",
      "epoch 321, loss function 3.5942957401275635\n",
      "epoch 322, loss function 3.5942957401275635\n",
      "epoch 323, loss function 3.5942957401275635\n",
      "epoch 324, loss function 3.5942957401275635\n",
      "epoch 325, loss function 3.5942957401275635\n",
      "epoch 326, loss function 3.5942957401275635\n",
      "epoch 327, loss function 3.5942957401275635\n",
      "epoch 328, loss function 3.5942957401275635\n",
      "epoch 329, loss function 3.5942957401275635\n",
      "epoch 330, loss function 3.5942957401275635\n",
      "epoch 331, loss function 3.5942957401275635\n",
      "epoch 332, loss function 3.5942957401275635\n",
      "epoch 333, loss function 3.5942957401275635\n",
      "epoch 334, loss function 3.5942957401275635\n",
      "epoch 335, loss function 3.5942957401275635\n",
      "epoch 336, loss function 3.5942957401275635\n",
      "epoch 337, loss function 3.5942957401275635\n",
      "epoch 338, loss function 3.5942957401275635\n",
      "epoch 339, loss function 3.5942957401275635\n",
      "epoch 340, loss function 3.5942957401275635\n",
      "epoch 341, loss function 3.5942957401275635\n",
      "epoch 342, loss function 3.5942957401275635\n",
      "epoch 343, loss function 3.5942957401275635\n",
      "epoch 344, loss function 3.5942957401275635\n",
      "epoch 345, loss function 3.5942957401275635\n",
      "epoch 346, loss function 3.5942957401275635\n",
      "epoch 347, loss function 3.5942957401275635\n",
      "epoch 348, loss function 3.5942957401275635\n",
      "epoch 349, loss function 3.5942957401275635\n",
      "epoch 350, loss function 3.5942957401275635\n",
      "epoch 351, loss function 3.5942957401275635\n",
      "epoch 352, loss function 3.5942957401275635\n",
      "epoch 353, loss function 3.5942957401275635\n",
      "epoch 354, loss function 3.5942957401275635\n",
      "epoch 355, loss function 3.5942957401275635\n",
      "epoch 356, loss function 3.5942957401275635\n",
      "epoch 357, loss function 3.5942957401275635\n",
      "epoch 358, loss function 3.5942957401275635\n",
      "epoch 359, loss function 3.5942957401275635\n",
      "epoch 360, loss function 3.5942957401275635\n",
      "epoch 361, loss function 3.5942957401275635\n",
      "epoch 362, loss function 3.5942957401275635\n",
      "epoch 363, loss function 3.5942957401275635\n",
      "epoch 364, loss function 3.5942957401275635\n",
      "epoch 365, loss function 3.5942957401275635\n",
      "epoch 366, loss function 3.5942957401275635\n",
      "epoch 367, loss function 3.5942957401275635\n",
      "epoch 368, loss function 3.5942957401275635\n",
      "epoch 369, loss function 3.5942957401275635\n",
      "epoch 370, loss function 3.5942957401275635\n",
      "epoch 371, loss function 3.5942957401275635\n",
      "epoch 372, loss function 3.5942957401275635\n",
      "epoch 373, loss function 3.5942957401275635\n",
      "epoch 374, loss function 3.5942957401275635\n",
      "epoch 375, loss function 3.5942957401275635\n",
      "epoch 376, loss function 3.5942957401275635\n",
      "epoch 377, loss function 3.5942957401275635\n",
      "epoch 378, loss function 3.5942957401275635\n",
      "epoch 379, loss function 3.5942957401275635\n",
      "epoch 380, loss function 3.5942957401275635\n",
      "epoch 381, loss function 3.5942957401275635\n",
      "epoch 382, loss function 3.5942957401275635\n",
      "epoch 383, loss function 3.5942957401275635\n",
      "epoch 384, loss function 3.5942957401275635\n",
      "epoch 385, loss function 3.5942957401275635\n",
      "epoch 386, loss function 3.5942957401275635\n",
      "epoch 387, loss function 3.5942957401275635\n",
      "epoch 388, loss function 3.5942957401275635\n",
      "epoch 389, loss function 3.5942957401275635\n",
      "epoch 390, loss function 3.5942957401275635\n",
      "epoch 391, loss function 3.5942957401275635\n",
      "epoch 392, loss function 3.5942957401275635\n",
      "epoch 393, loss function 3.5942957401275635\n",
      "epoch 394, loss function 3.5942957401275635\n",
      "epoch 395, loss function 3.5942957401275635\n",
      "epoch 396, loss function 3.5942957401275635\n",
      "epoch 397, loss function 3.5942957401275635\n",
      "epoch 398, loss function 3.5942957401275635\n",
      "epoch 399, loss function 3.5942957401275635\n",
      "epoch 400, loss function 3.5942957401275635\n",
      "epoch 401, loss function 3.5942957401275635\n",
      "epoch 402, loss function 3.5942957401275635\n",
      "epoch 403, loss function 3.5942957401275635\n",
      "epoch 404, loss function 3.5942957401275635\n",
      "epoch 405, loss function 3.5942957401275635\n",
      "epoch 406, loss function 3.5942957401275635\n",
      "epoch 407, loss function 3.5942957401275635\n",
      "epoch 408, loss function 3.5942957401275635\n",
      "epoch 409, loss function 3.5942957401275635\n",
      "epoch 410, loss function 3.5942957401275635\n",
      "epoch 411, loss function 3.5942957401275635\n",
      "epoch 412, loss function 3.5942957401275635\n",
      "epoch 413, loss function 3.5942957401275635\n",
      "epoch 414, loss function 3.5942957401275635\n",
      "epoch 415, loss function 3.5942957401275635\n",
      "epoch 416, loss function 3.5942957401275635\n",
      "epoch 417, loss function 3.5942957401275635\n",
      "epoch 418, loss function 3.5942957401275635\n",
      "epoch 419, loss function 3.5942957401275635\n",
      "epoch 420, loss function 3.5942957401275635\n",
      "epoch 421, loss function 3.5942957401275635\n",
      "epoch 422, loss function 3.5942957401275635\n",
      "epoch 423, loss function 3.5942957401275635\n",
      "epoch 424, loss function 3.5942957401275635\n",
      "epoch 425, loss function 3.5942957401275635\n",
      "epoch 426, loss function 3.5942957401275635\n",
      "epoch 427, loss function 3.5942957401275635\n",
      "epoch 428, loss function 3.5942957401275635\n",
      "epoch 429, loss function 3.5942957401275635\n",
      "epoch 430, loss function 3.5942957401275635\n",
      "epoch 431, loss function 3.5942957401275635\n",
      "epoch 432, loss function 3.5942957401275635\n",
      "epoch 433, loss function 3.5942957401275635\n",
      "epoch 434, loss function 3.5942957401275635\n",
      "epoch 435, loss function 3.5942957401275635\n",
      "epoch 436, loss function 3.5942957401275635\n",
      "epoch 437, loss function 3.5942957401275635\n",
      "epoch 438, loss function 3.5942957401275635\n",
      "epoch 439, loss function 3.5942957401275635\n",
      "epoch 440, loss function 3.5942957401275635\n",
      "epoch 441, loss function 3.5942957401275635\n",
      "epoch 442, loss function 3.5942957401275635\n",
      "epoch 443, loss function 3.5942957401275635\n",
      "epoch 444, loss function 3.5942957401275635\n",
      "epoch 445, loss function 3.5942957401275635\n",
      "epoch 446, loss function 3.5942957401275635\n",
      "epoch 447, loss function 3.5942957401275635\n",
      "epoch 448, loss function 3.5942957401275635\n",
      "epoch 449, loss function 3.5942957401275635\n",
      "epoch 450, loss function 3.5942957401275635\n",
      "epoch 451, loss function 3.5942957401275635\n",
      "epoch 452, loss function 3.5942957401275635\n",
      "epoch 453, loss function 3.5942957401275635\n",
      "epoch 454, loss function 3.5942957401275635\n",
      "epoch 455, loss function 3.5942957401275635\n",
      "epoch 456, loss function 3.5942957401275635\n",
      "epoch 457, loss function 3.5942957401275635\n",
      "epoch 458, loss function 3.5942957401275635\n",
      "epoch 459, loss function 3.5942957401275635\n",
      "epoch 460, loss function 3.5942957401275635\n",
      "epoch 461, loss function 3.5942957401275635\n",
      "epoch 462, loss function 3.5942957401275635\n",
      "epoch 463, loss function 3.5942957401275635\n",
      "epoch 464, loss function 3.5942957401275635\n",
      "epoch 465, loss function 3.5942957401275635\n",
      "epoch 466, loss function 3.5942957401275635\n",
      "epoch 467, loss function 3.5942957401275635\n",
      "epoch 468, loss function 3.5942957401275635\n",
      "epoch 469, loss function 3.5942957401275635\n",
      "epoch 470, loss function 3.5942957401275635\n",
      "epoch 471, loss function 3.5942957401275635\n",
      "epoch 472, loss function 3.5942957401275635\n",
      "epoch 473, loss function 3.5942957401275635\n",
      "epoch 474, loss function 3.5942957401275635\n",
      "epoch 475, loss function 3.5942957401275635\n",
      "epoch 476, loss function 3.5942957401275635\n",
      "epoch 477, loss function 3.5942957401275635\n",
      "epoch 478, loss function 3.5942957401275635\n",
      "epoch 479, loss function 3.5942957401275635\n",
      "epoch 480, loss function 3.5942957401275635\n",
      "epoch 481, loss function 3.5942957401275635\n",
      "epoch 482, loss function 3.5942957401275635\n",
      "epoch 483, loss function 3.5942957401275635\n",
      "epoch 484, loss function 3.5942957401275635\n",
      "epoch 485, loss function 3.5942957401275635\n",
      "epoch 486, loss function 3.5942957401275635\n",
      "epoch 487, loss function 3.5942957401275635\n",
      "epoch 488, loss function 3.5942957401275635\n",
      "epoch 489, loss function 3.5942957401275635\n",
      "epoch 490, loss function 3.5942957401275635\n",
      "epoch 491, loss function 3.5942957401275635\n",
      "epoch 492, loss function 3.5942957401275635\n",
      "epoch 493, loss function 3.5942957401275635\n",
      "epoch 494, loss function 3.5942957401275635\n",
      "epoch 495, loss function 3.5942957401275635\n",
      "epoch 496, loss function 3.5942957401275635\n",
      "epoch 497, loss function 3.5942957401275635\n",
      "epoch 498, loss function 3.5942957401275635\n",
      "epoch 499, loss function 3.5942957401275635\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000): \n",
    "    predict_y = linear_model(Sample_X_data) \n",
    "    loss = criterion(predict_y, Sample_Y_data) \n",
    "\n",
    "\n",
    "    Optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "\n",
    "    Ooptimizer.step() \n",
    "    print('epoch {}, loss function {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc890ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_variable = torch.randn(1, 3, requires_grad=True)\n",
    "\n",
    "predict_y = linear_model(test_variable)\n",
    "predict_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f290f",
   "metadata": {},
   "source": [
    "# Now I want to go deeper into this topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22313955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in c:\\users\\masoumifard.kh\\anaconda3\\lib\\site-packages (1.6.3)\n",
      "Requirement already satisfied: cftime in c:\\users\\masoumifard.kh\\anaconda3\\lib\\site-packages (from netCDF4) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\masoumifard.kh\\anaconda3\\lib\\site-packages (from netCDF4) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4fa52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bd3b8c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mmake_regression(\n\u001b[0;32m      2\u001b[0m     n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "X, y = datasets.make_regression(\n",
    "    n_samples=1000, n_features=10, noise=5, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ab21e",
   "metadata": {},
   "source": [
    "# batch \n",
    "* https://deeplizard.com/learn/video/U4WB9p6ODjM\n",
    "* https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/\n",
    "* https://www.baeldung.com/cs/epoch-vs-batch-vs-mini-batch\n",
    "* https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "* https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
    "* https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf587501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
